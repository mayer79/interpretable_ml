---
title: "Connecting descriptive analyses with machine learning"
date: "`r Sys.Date()`"
author: Michael Mayer
output:
  html_document:
    toc: true
---

```{r setup, warning = FALSE, message = FALSE, echo = FALSE}
source("../r/functions.R", encoding = "UTF-8")

knitr::opts_chunk$set(fig.width = 16, fig.height = 10, echo = FALSE, 
                      warning = FALSE, message = FALSE) 

# Path to data
path <- "../rdata"

# Load prep data plus further constants
load(file.path(path, "prep.RData"))
load(file.path(path, "models.RData"))

library(tidyverse)
library(ggpubr)
library(moderndive)
library(kableExtra)

```
In contrast to classic statistical modelling techniques like linear regression, modern machine learning approaches tend to provide black box results. In some areas like visual computing or natural language processing, this is not an issue since there, focus usually lies on predicting things. Either the model provides sufficiently useful predictions in practice or it will be discarded. However, in areas where the purpose of a statistical model is also to explain or validate underlying theories (e.g. in medicine, economics, and biologiy), black box models are of little use. Thus, there is need to make machine learning models less black. Special focus is on "model agnostic" approaches that work for any kind of modelling technique. They keep the workflow simple.

There are different types of model agnostic aspects of a model, e.g.

- variable importance through random permutation,

- ceteris paribus profiles: how do predictions of object $i$ change when systematically changing the values of variable $X$?

- marginal effects plots: Aggregate ceteris paribus profiles across many observations to find average partial effects.

- Model performance

In R, the `DALEX` package provides a common interface to many such tools. It works by creating an explainer consisting of a model fit, an evaluation data set, a prediction function based on the latter two, the responses, a label and an inverse link function to post-process the predictions. Based on these, all model aganostic methods can be derived as they strictly require only a predict method. One disadvantage of `DALEX` is that its results are not meant for postprocessing. E.g. it is not easy to combine the results of partial dependence plots with empirical effects. In my view, the latter is the core to many good presentations: How does the response depend on a factor $X$ empirically as well as modelled? How does typical house prices depend on the lot size (empirically as well as conditionally on other factors)?

The aim of this project is to illustrate some of the ideas and to provide a simple template for your own machine learning projects.

## Data

We use the data set `house_prices` with information on `r format_large(nrow(prep))` houses sold in King County between May 2014 and May 2015. It is shipped along with R package `moderndive`.

The first few observations look as follows:

```{r}
house_prices %>% 
  head(10) %>% 
  kable() %>%
  kable_styling() %>% 
  row_spec(row = 1:10, color = "white", background = "#D7261E")
```


## Models and their performance

We have considered 

- linear regression,

- random forests and

- boosted trees

to model house prices at logarithmic level. 

The models were calculated on 70% of the data (training data) and were validated and interpreted on 20% (validation data). 10% are still untouched for modelling (test data). The validation data set has not been used otherwise for modelling, so e.g. the parameters of the boosted trees were chosen by five-fold cross-validation on the *training* data.

### Monotonicity

One big step towards explainability is to introduce monotonicity constraints, an option that is e.g. available in the tree boosting libraries "XGBoost" and "LightGBM" (we are working here with the latter). We have imposed positive constraints on "grade", "sqft_living", "condition" as well as "waterfront". As a consequence, ceteris paribus profiles and their aggregates are monotonically increasing in these variables (keeping all other factors constant).

### Performance

We have ended up with the following metrics evaluated on the validation data in the log world. 

```{r}
ggplot(perfTable, aes(x = label, y = value, fill = label)) + 
  geom_bar(position = position_dodge(), stat = "identity", show.legend = FALSE) +
  facet_wrap(~type) +
  ggtitle("Model performance") + 
  theme_bw(base_size = 25) +
  theme(legend.title = element_blank(), 
        legend.position = "bottom", 
        axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(x = element_blank(), y = element_blank()) +
  scale_fill_viridis_d(begin = 0.1, end = 0.7)
```

## Association of variables with response

The following diagram shows increase in mean squared error loss due to random shuffling each variable. The more important the variable, the larger the loss increase. A completely irrelevant variable can be shuffled without impact on the prediction accuracy.

```{r}
ggplot(varImp, aes(x = reorder(variable, dropout_loss), y = dropout_loss, fill = label)) + 
  geom_bar(position = position_dodge(), stat = "identity", show.legend = FALSE) +
  facet_wrap(~label) +
  coord_flip() +
  theme_bw(base_size = 25) +
  theme(axis.text.x = element_blank()) +
  labs(x = element_blank(), y = element_blank()) +
  scale_fill_viridis_d(begin = 0.1, end = 0.7)
```

Best predictors are `r paste(most_relevant[1:4], collapse = ", ")`, and `r most_relevant[5]`.

## Effect per variable

We now describe the association for each covariable by considering

- **empirical effects**, i.e. boxplots (boxes only) of the response per level of the covariable,

- corresponding partial dependence plots. They show **modelled average effects** that cannot be explained by other variables.

The results are ordered by variable importance.

```{r, results = "asis"}
for (v in most_relevant) { # v <- most_relevant[[1]]
  cat(sprintf("### %s \n", v))
  
  resp <- ggplot(mapping = aes(x = factor(temp_), y = price)) +
    stat_summary(data = eff[[v]]$data, fun.data = box_stats, geom = "crossbar",
                 width = 0.3, fill = "darkblue", colour = "black", alpha = 0.1) +
    geom_line(data = eff[[v]]$modelled, aes(group = label, color = label), size = 1.5) +
    geom_point(data = eff[[v]]$modelled, aes(group = label, color = label), size = 4) +
    theme_bw(base_size = 25) +
    theme(legend.position = c(0.5, 0.9), legend.title = element_blank(),
          legend.background = element_rect(fill = "transparent"),
          axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1)) +
    xlab(element_blank()) +
    scale_y_continuous(name = "Price", labels = format_large) +
    scale_color_viridis_d(begin = 0.1, end = 0.7)
  
  counts <- ggplot(count(eff[[v]]$data, temp_), aes(x = temp_, y = n, label = format_large(n))) +
    geom_bar(stat = "identity", fill = "darkblue", alpha = 0.1, width = 0.3) +
    geom_text(aes(y = 0), angle = 90, hjust = -0.1, size = 6) +
    theme_void() +
    theme(strip.text.x = element_blank(), panel.grid = element_blank())

  print(ggarrange(counts, resp, heights = c(0.2, 1), ncol = 1, nrow = 2, align = "v"))
  
  cat("\n\n")
}
```
