---
title: "Monotone Constraints in Tree Boosting Models"
output:
  html_document:
    df_print: paged
---

## Initializing stuff

```{r}
options(warn=-1)

library(moderndive) # For data 
library(gbm)
library(xgboost)
library(lightgbm) # From github
library(DALEX) # For model interpretation
library(ceterisParibus)
library(tidyverse) # For data prep
library(caret) # For data split

rmse <- function(y, pred) {
  sqrt(sum((y - pred)^2))
}

dim(house_prices)

# Apply necessary data preparation steps
prep <- transform(house_prices, 
                  lPrice = log(price),
                  year = lubridate::year(date),
                  age = lubridate::year(date) - yr_built,
                  dRenovated = 0 + (yr_renovated > 0),
                  log_sqft_lot = log(sqft_lot),
                  zipcode = as.integer(zipcode),
                  condition = as.integer(condition),
                  grade = as.integer(grade))

# Some column names
x <- c("year", "age", "bedrooms", "bathrooms", "sqft_living", "log_sqft_lot", 
            "dRenovated", "view", "condition", "grade", "zipcode", "lat", "long")
y <- "lPrice"


# Train/test split
set.seed(3928272)
ind <- caret::createDataPartition(prep[[y]], p = 0.80, list = FALSE) %>% c

trainDF <- prep[ind, c(x, y)]
validDF <- prep[-ind, c(x, y)]

trainMat <- as.matrix(trainDF)
validMat <- as.matrix(validDF)

trainXgb <- xgb.DMatrix(trainMat[, x], label = trainMat[, y])
trainLgb <- lgb.Dataset(trainMat[, x], label = trainMat[, y])
```


## Fit the models: Linear model, boosted trees, random forest
```{r}
form <- reformulate(x, y)
mc <- list(c(0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0))
#params_constrained['monotone_constraints'] = "(1,-1)"

params_gbm <- list(
  form, 
  data = trainDF, 
  n.trees = 200, 
  interaction.depth = 5, 
  shrinkage = 0.05)

gbm_unconstrained <- do.call(gbm, params_gbm)
gbm_constrained <- do.call(gbm, c(params_gbm, var.monotone = mc))

# Tree booster: XGBoost
param_xgb <- list(max_depth = 5, 
                  learning_rate = 0.05, 
                  nthread = 4, 
                  objective = "reg:linear")

(fit_xgb <- xgb.train(param_xgb,
                      data = trainXgb,
                      nrounds = 500))
```

## Initializing the "explainer"
```{r}
explainer_gbm_unconstrained <- explain(gbm_unconstrained, 
                         data = validDF[x], 
                         y = validDF[[y]], 
                         label = "gbm_unconstrained",
                         predict_function = function(model, x) predict(model, x, n.trees = 200))
explainer_gbm_constrained <- explain(gbm_constrained, 
                         data = validDF[x], 
                         y = validDF[[y]], 
                         label = "gbm_constrained",
                         predict_function = function(model, x) predict(model, x, n.trees = 200))
explainers <- list(explainer_gbm_unconstrained, explainer_gbm_constrained)
```

## Effect of grade: ceteris paribus ("everything else being fixed") profiles across models

Here, we pick one or more individual observations and check how their predictions
change if the value of a selected variable is systematically being changed.
For an additive linear model, the movement is identical for all observations.
These plots make sense as long as ceteris paribus interpretations make sense
(i.e. if no strong causal relationships to other regressors exist.)

```{r}
x_selected <- c("sqft_living", "log_sqft_lot")

picks <- 1:20
cp <- ceteris_paribus(explainer_gbm_unconstrained, 
                      observations = validDF[picks, x])
plot(cp, selected_variables = x_selected)
```

## Effect of driver age: multiple ceteris paribus profiles for the XGBoost model
picks <- 1:20
cp <- ceteris_paribus(explainer_xgb, observations = validDF[picks, x])
plot(cp, selected_variables = x_selected)

## Effect of driver age: average of above profiles aka "partial dependence plots"
# If multiple ceteris paribus profiles are being aggregated, we end up with 
# Friedman's famous partial dependence plots. 

plot(cp, selected_variables = x_selected, aggregate_profiles = mean)


## Relationship between living area and price: partial dependency plots across models
```{r}
x_selected <- "sqft_living"

cp <- lapply(explainers, 
             variable_response, 
             variable = x_selected)

do.call(plot, cp)
```

## Relationship between lot size and price: partial dependency plots across models
```{r}
x_selected <- "log_sqft_lot"

cp <- lapply(explainers, 
             variable_response, 
             variable = x_selected)

do.call(plot, cp)
```